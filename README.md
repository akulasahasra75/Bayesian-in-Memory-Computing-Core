# Bayesian-in-Memory-Computing-Core
This project presents a fully digital implementation of a Bayesian In-Memory Computing (IMC) core that enables Binarized Neural Networks (BNNs) to perform probabilistic inference with inherent uncertainty quantification. The design integrates computation directly within memory arrays to overcome the von Neumann bottleneck while maintaining Bayesian reasoning capabilities.


# **Theory of Operation**
## **Bayesian Inference Framework**
The core implements a hardware realization of Bayesian inference through Monte Carlo sampling. Unlike traditional deterministic neural networks that produce single-point estimates, our Bayesian approach maintains a probability distribution over weights, enabling the system to quantify uncertainty in its predictions.

## **Probabilistic Weight Representation**
Each weight in the system is represented as a pair (w_ij, c_ij), where w_ij is the base weight value and c_ij ∈ [0,1] represents the confidence in that weight. High confidence values indicate well-determined parameters, while low values represent uncertain weights that should be explored during sampling.

## **Monte Carlo Sampling Process**
The system performs hardware-based Monte Carlo integration by drawing multiple samples from the weight posterior distribution. For each inference request, the core generates 8 different weight configurations by perturbing the base weights according to their confidence values. This sampling process approximates the Bayesian predictive distribution by averaging over multiple plausible weight configurations.

## **Weight Perturbation Mechanism**
During the perturbation phase, each weight bit is conditionally flipped based on its confidence level and random noise generated by the LFSR. Weights with high confidence (c_ij ≈ 1) remain stable across samples, while uncertain weights (c_ij ≈ 0) vary significantly, exploring different hypotheses about their true values.

## **Statistical Analysis and Confidence Scoring**
After collecting all samples, the system computes both the mean similarity score and a confidence measure. The mean represents the expected prediction, while the confidence score quantifies the uncertainty by measuring the variance across samples. Low variance indicates high confidence in the prediction, while high variance suggests the model is uncertain.

## **In-Memory Computation Paradigm**
The architecture eliminates the von Neumann bottleneck by performing XNOR similarity computations and population counts directly within the memory interface. This approach significantly reduces data movement between memory and processing units, making it particularly efficient for edge AI applications with strict power constraints.

# **System Block Diagram**
<img width="1234" height="920" alt="image" src="https://github.com/user-attachments/assets/7e2d6894-bee8-4f31-96a0-c3aa4995b38d" />


# **Key Features**
-Hardware-based Monte Carlo Sampling: Implements probabilistic inference through 8 parallel samples
-Uncertainty Quantification: Produces both mean predictions and confidence scores
-In-Memory Computing: Reduces data movement by integrating computation within memory
-Synthesizable Verilog: Fully digital implementation compatible with standard EDA tools
-48-Cycle Inference: Predictable timing with complete inference in 48 clock cycles
-Dynamic Weight Perturbation: LFSR-based random weight modification based on confidence levels


# **Finite State Machine**
<img width="1312" height="869" alt="image" src="https://github.com/user-attachments/assets/a5d613bc-0937-4999-bd5e-3910cd5808e7" />


# **Simulation Results**
The waveform shows:
-State transitions through the 8-state FSM
-Weight perturbation based on confidence patterns
-Population count results for each sample
-Final mean and confidence outputs
-48-cycle inference completion

<img width="1714" height="854" alt="image" src="https://github.com/user-attachments/assets/85214817-1f87-45f3-8b3f-d08a6ab7c067" />


# **Conclusion and Future Work**
This project successfully demonstrates a Bayesian In-Memory Computing core that bridges computational efficiency with uncertainty awareness for edge AI applications. The digital implementation provides both predictions and confidence scores, making it suitable for safety-critical systems where reliability estimation is essential.

Future work will focus on enhanced perturbation logic with proper stochastic sampling, FPGA prototyping for hardware validation, and architectural scaling to larger networks. Additional improvements include advanced statistical analysis, power optimization, and domain-specific adaptations for medical and automotive applications. This foundation enables further exploration of uncertainty-aware computing in resource-constrained environments.





